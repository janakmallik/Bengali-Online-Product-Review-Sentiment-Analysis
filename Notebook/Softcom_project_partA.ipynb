{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VUhjcVa6EKA"
      },
      "source": [
        "**code after reducing** -> punctuation, numbers, stopwords, emojis, English words\n",
        "<!-- Lemmatized -->\n",
        "**keeping** ->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORzIIO0Pggql",
        "outputId": "91fbda8e-5c63-4481-b689-a37e965780b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUJi3OCScnzC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import re,json,nltk\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report,accuracy_score,precision_score,recall_score,f1_score\n",
        "#stopwords_list ='stopwords-bn.txt'\n",
        "\n",
        "\n",
        "import re\n",
        "from google.colab import files\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpE_1eIMJiJ1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsWxktacd4gh"
      },
      "outputs": [],
      "source": [
        "data = pd.read_excel('/content/drive/MyDrive/4.2_resources/softcom/data')\n",
        "print(\"Total Reviews:\",len(data),\n",
        "      \"\\nTotal Positive Reviews:\",len(data[data.Sentiment == 1]),\n",
        "      \"\\nTotal Neutral Reviews:\",len(data[data.Sentiment == 2]),\n",
        "      \"\\nTotal Negative Reviews:\",len(data[data.Sentiment == 0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRJJUAeAeEz5"
      },
      "outputs": [],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orb7yYmhfHZ4"
      },
      "outputs": [],
      "source": [
        "# print some unprocessed reviews\n",
        "sample_data = [573, 1209, 864, 297, 1502, 639, 2085, 431, 775, 1926, 1098, 256, 1823, 904, 117, 1985, 623, 1490, 765, 2217]\n",
        "for i in sample_data:\n",
        "      print(data.Comment[i],'\\n','Sentiment:-- ',data.Tag[i],'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avr0FtS5eI6Z"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', str(text))\n",
        "\n",
        "    # Remove all punctuation using string.punctuation\n",
        "    # review = re.sub('[^\\u0980-\\u09FF]',' ',str(review)) #removing unnecessary punctuation\n",
        "    text = re.sub('[^\\u0980-\\u09FF]',' ',str(text))  # Keeping Bengali characters and specified punctuation\n",
        "\n",
        "    # Remove English words\n",
        "    text = re.sub(r'[A-Za-z]+', '', str(text))\n",
        "\n",
        "    # Remove emojis\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\"\n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', str(text))\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdhGVUpQeKpZ"
      },
      "outputs": [],
      "source": [
        "# Apply the function to the dataframe\n",
        "\n",
        "data['cleaned'] = data['Comment'].apply(clean_text)\n",
        "\n",
        "# Print some cleaned reviews from the dataset\n",
        "sample_data = [573, 1209, 864, 297, 1502, 639, 2085, 431, 775, 1926, 1098, 256, 1823, 904, 117, 1985, 623, 1490, 765, 2217]\n",
        "\n",
        "for i in sample_data:\n",
        "    print('Original:\\n', data.Comment[i], '\\nCleaned:\\n', data.cleaned[i], '\\n', 'Sentiment:-- ', data.Tag[i], '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvNyeBm1eGfx"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load Bengali stopwords\n",
        "stop_words = set(stopwords.words('bengali'))\n",
        "\n",
        "# Function to remove stopwords from text\n",
        "def remove_stopwords(text):\n",
        "    words = text.split()  # Split the text into words\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]  # Remove stopwords\n",
        "    return ' '.join(filtered_words)  # Recreate text without stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wW4269aaoIv3",
        "outputId": "46eaa979-5809-44e9-d415-13b623fa1fe4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'যাওয়া', 'ছাড়াও', 'কমনে', 'যাতে', 'পক্ষে', 'এত', 'জন্যওজে', 'কাউকে', 'দেয়', 'চার', 'হলেই', 'কিন্তু', 'পেয়ে', 'যেন', 'রেখে', 'ই', 'যাদের', 'এটা', 'করার', 'করতে', 'কেন', 'হয়ে', 'মধ্যে', 'দেওয়া', 'যাঁর', 'পি', 'আপনি', 'বক্তব্য', 'তো', 'যার', 'কত', 'নিজেদের', 'প্রভৃতি', 'এতটাই', 'বরং', 'যতটা', 'হবে', 'নাকি', 'পরেই', 'চেয়ে', 'দিতে', 'অন্য', 'কিংবা', 'সেটাই', 'দেওয়া', 'হত', 'বললেন', 'বলা', 'এল', 'ফলে', 'ওখানে', 'পাচ', 'মতো', 'কোনও', 'শুরু', 'অবধি', 'স্পষ্ট', 'হয়নি', 'হইতে', 'থাকবেন', 'হতেই', 'দিয়েছেন', 'নানা', 'তুমি', 'এখানেই', 'সি', 'আগেই', 'আজ', 'তাঁরা', 'করলে', 'ধরা', 'সম্প্রতি', 'জানানো', 'ওদের', 'কাজ', 'কোনো', 'বি', 'হইবে', 'প্রায়', 'তারৈ', 'আছে', 'কেউ', 'করেন', 'তাহা', 'তবে', 'বিনা', 'কয়েক', 'সেটি', 'ফের', 'যখন', 'বলতে', 'প্রযন্ত', 'অবশ্য', 'জনকে', 'নেওয়া', 'সুতরাং', 'হতে', 'জানা', 'এর', 'তাদের', 'দ্বারা', 'গিয়েছে', 'হয়তো', 'রাখা', 'করে', 'ভাবে', 'আগে', 'ওঁদের', 'জানায়', 'এদের', 'দিকে', 'র', 'তারা', 'মাধ্যমে', 'গেল', 'এব', 'করেছিলেন', 'সঙ্গেও', 'এঁরা', 'সামনে', 'আমরা', 'তিনিও', 'নয়', 'এটি', 'পর', 'করায়', 'তথা', 'করাই', 'বহু', 'যেতে', 'তখন', 'কাজে', 'এই', 'নেওয়া', 'তিনি', 'পর্যন্ত', 'শুধু', 'অন্তত', 'তাহাতে', 'তারপর', 'এমন', 'সেই', 'বাদে', 'অথবা', 'দেখা', 'কাছ', 'যায়', 'জানতে', 'এক্', 'থাকে', 'থেকেও', 'অর্থাত', 'দিন', 'দেখতে', 'কী', 'হয়েছে', 'বন', 'বলে', 'করছে', 'উপরে', 'ফিরে', 'হচ্ছে', 'তাও', 'এরা', 'তাই', 'মতোই', 'দু', 'যাওয়ার', 'মধ্যেই', 'হয়েই', 'থাকা', 'নাগাদ', 'করলেন', 'ব্যাপারে', 'তাঁর', 'করবে', 'তাতে', 'ইহা', 'ভাবেই', 'তোমার', 'তার', 'এবং', 'হইয়া', 'নিতে', 'কি', 'পারেন', 'চেষ্টা', 'তা', 'মাত্র', 'থাকেন', 'তত', 'সেটাও', 'নিয়ে', 'আগামী', 'হলেও', 'থাকবে', 'চায়', 'যদি', 'অনেকে', 'এ', 'বার', 'তিনঐ', 'উপর', 'আই', 'কিছু', 'ও', 'বিভিন্ন', 'এতে', 'কোন', 'জানিয়েছে', 'বিশেষ', 'গুলি', 'সাধারণ', 'আরও', 'করেছে', 'ছাড়া', 'করেই', 'অনেকেই', 'থাকায়', 'গোটা', 'সে', 'যত', 'সমস্ত', 'তাঁদের', 'হওয়ার', 'জ্নজন', 'বেশ', 'থেকেই', 'অতএব', 'সব', 'ধামার', 'হল', 'যেখানে', 'দুটি', 'হওয়ায়', 'সেখানে', 'যদিও', 'ঠিক', 'দুটো', 'যাওয়া', 'পরে', 'আর', 'এখানে', 'সহিত', 'প্রতি', 'ওর', 'যা', 'সঙ্গে', 'যাকে', 'আবার', 'যে', 'দেওয়ার', 'যাবে', 'মোটেই', 'তাহলে', 'দেখে', 'কখনও', 'তাঁাহারা', 'ওই', 'স্বয়ং', 'গেছে', 'বদলে', 'ওকে', 'কেখা', 'টি', 'রয়েছে', 'দিলেন', 'হৈলে', 'তাহার', 'যান', 'করিয়ে', 'হয়েছেন', 'ঐ', 'এঁদের', 'বেশি', 'কাছে', 'দেন', 'আপনার', 'ছিলেন', 'করিতে', 'পারি', 'পারে', 'নেই', 'হয়েছিল', 'কয়েক', 'জে', 'যাচ্ছে', 'যিনি', 'বসে', 'হয়', 'মধ্যেও', 'হওয়া', 'হবেন', 'নাই', 'প্রথম', 'পরেও', 'চলে', 'একই', 'কে', 'চালু', 'লক্ষ', 'হলো', 'রকম', 'দুই', 'অথচ', 'আমার', 'হাজার', 'করছেন', 'অনেক', 'এস', 'এমনকী', 'বা', 'জানিয়ে', 'একটি', 'নেওয়ার', 'ব্যবহার', 'জন', 'করিয়া', 'এখন', 'মোট', 'করবেন', 'বলল', 'গিয়ে', 'পেয়্র্', 'প্রায়', 'গেলে', 'থেকে', 'করি', 'প্রাথমিক', 'বলেন', 'ইত্যাদি', 'করা', 'না', 'ওঁরা', 'কয়েকটি', 'আমাকে', 'ধরে', 'বিষয়টি', 'এটাই', 'করেছেন', 'মনে', 'যেমন', 'মধ্যভাগে', 'অনুযায়ী', 'দিয়েছে', 'সেটা', 'যথেষ্ট', 'তেমন', 'এবার', 'যাঁরা', 'হন', 'হলে', 'নিজেই', 'হয়', 'একে', 'জনের', 'একবার', 'তবু', 'চান', 'উচিত', 'যারা', 'উনি', 'এমনি', 'দিয়ে', 'খুব', 'বলেছেন', 'সহ', 'জন্য', 'নতুন', 'সবার', 'পাওয়া', 'উত্তর', 'তুলে', 'কিছুই', 'তাঁকে', 'ওঁর', 'এখনও', 'কারও', 'নয়', 'নিজের', 'হোক', 'এসে', 'ওরা', 'গিয়ে', 'হিসাবে', 'কোটি', 'আদ্যভাগে', 'তাকে', 'নিয়ে', 'নিজে', 'ছিল', 'আমি', 'কারণ', 'আমাদের', 'সেখান', 'কেউই', 'ক্ষেত্রে', 'কবে'}\n"
          ]
        }
      ],
      "source": [
        "print(stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbYl8s-elNah"
      },
      "outputs": [],
      "source": [
        "# Apply the function to the dataframe\n",
        "data['cleaned_stopwords'] = data['cleaned'].apply(remove_stopwords)\n",
        "\n",
        "# Print some cleaned reviews from the dataset\n",
        "sample_data = [573, 1209, 864, 297, 1502, 639, 2085, 431, 775, 1926, 1098, 256, 1823, 904, 117, 1985, 623, 1490, 765, 2217]\n",
        "\n",
        "for i in sample_data:\n",
        "    print('cleaned:\\n', data.cleaned[i], '\\ncleaned_stopwords:\\n', data.cleaned_stopwords[i], '\\n', 'Sentiment:-- ', data.Tag[i], '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install BnLemma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import BnLemma as lm\n",
        "\n",
        "# Function to remove stopwords from text\n",
        "def ben_Lemmatizer(text):\n",
        "    bl = lm.Lemmatizer()\n",
        "    return bl.lemma(text)\n",
        "\n",
        "\n",
        "data['Lemmatized'] = data['cleaned'].apply(ben_Lemmatizer)\n",
        "\n",
        "\n",
        "\n",
        "# Example print after removing stopwords\n",
        "for i in sample_data:\n",
        "      print('Original:\\n',data.cleaned[i],'\\nCleaned:\\n',data.Lemmatized[i],'\\n','Sentiment:-- ',data.Sentiment[i],'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdN2P8ipopUO"
      },
      "outputs": [],
      "source": [
        "#should we remove low length data...?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZR8rEe9ovs-"
      },
      "outputs": [],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbqBijcxug6f"
      },
      "outputs": [],
      "source": [
        "#pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfLrJryVYA1I"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'data' is your DataFrame\n",
        "selected_columns = ['cleaned_stopwords', 'Tag']\n",
        "selected_data = data[selected_columns]\n",
        "\n",
        "# Save the selected columns to a CSV file\n",
        "selected_data.to_csv('/content/drive/MyDrive/4.2_resources/softcom/data/project_selected_dataA.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFFXNHcpYd84"
      },
      "outputs": [],
      "source": [
        "cleaned_data = pd.read_csv('/content/drive/MyDrive/4.2_resources/softcom/data/project_selected_dataA.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWYonhaeY0cQ"
      },
      "outputs": [],
      "source": [
        "print(cleaned_data.columns)\n",
        "print(\"Total Reviews:\",len(cleaned_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Egi-9n0mct9A"
      },
      "outputs": [],
      "source": [
        "cleaned_data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PnHSVRE3S0a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "\n",
        "feature_columns = ['cleaned_stopwords']\n",
        "\n",
        "# X = cleaned_data[feature_columns].iloc[:1000]\n",
        "# y = cleaned_data['Tag'].iloc[:1000]\n",
        "\n",
        "X = cleaned_data[feature_columns]\n",
        "y = cleaned_data['Tag']\n",
        "\n",
        "label_mapping = {'Negative': 0, 'Positive': 1, 'Neutral': 2}\n",
        "y_encoded = y.map(label_mapping)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "batch_size = 2\n",
        "tokenized_data = []\n",
        "for i in range(0, len(X), batch_size):\n",
        "    batch_texts = X['cleaned_stopwords'].iloc[i:i+batch_size].tolist()\n",
        "    batch_encoding = tokenizer.batch_encode_plus(\n",
        "        batch_texts,\n",
        "        add_special_tokens=True,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    tokenized_data.append(batch_encoding)\n",
        "\n",
        "input_ids = torch.cat([d['input_ids'] for d in tokenized_data], dim=0)\n",
        "attention_mask = torch.cat([d['attention_mask'] for d in tokenized_data], dim=0)\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "bert_embeddings = outputs.last_hidden_state[:, 0, :]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZalLNbePZLUP"
      },
      "outputs": [],
      "source": [
        "len(bert_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TusLJWShsP_"
      },
      "outputs": [],
      "source": [
        "torch.save(bert_embeddings, '/content/drive/MyDrive/4.2_resources/softcom/data/bert_embeddingsA.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqU9cWSph8US"
      },
      "outputs": [],
      "source": [
        "loaded_embeddings = torch.load('/content/drive/MyDrive/4.2_resources/softcom/data/bert_embeddingsA.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiFMD2qUy4I8"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzijUnnUik-e"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "save_dict = {\n",
        "    'bert_embeddings': bert_embeddings,\n",
        "    'y_encoded': y_encoded\n",
        "}\n",
        "\n",
        "with open('/content/drive/MyDrive/4.2_resources/softcom/data/bert_and_y_encodedA.pkl', 'wb') as file:\n",
        "    pickle.dump(save_dict, file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeNhKLq5i7Be"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/4.2_resources/softcom/data/bert_and_y_encodedA.pkl', 'rb') as file:\n",
        "    loaded_dict = pickle.load(file)\n",
        "\n",
        "loaded_bert_embeddings = loaded_dict['bert_embeddings']\n",
        "loaded_y_encoded = loaded_dict['y_encoded']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLHIactApnZV"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wmbMiKB1CmA"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "def batch_train(model, X_train_batches, y_train_batches, X_test, y_test):\n",
        "    for X_batch, y_batch in zip(X_train_batches, y_train_batches):\n",
        "        model.fit(X_batch, y_batch)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "    return accuracy, classification_rep\n",
        "\n",
        "# Assuming loaded_bert_embeddings and loaded_y_encoded are defined\n",
        "X_train, X_test, y_train, y_test = train_test_split(loaded_bert_embeddings, loaded_y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "X_train_batches = [X_train[i:i+batch_size] for i in range(0, len(X_train), batch_size)]\n",
        "y_train_batches = [y_train[i:i+batch_size] for i in range(0, len(y_train), batch_size)]\n",
        "\n",
        "# K-Nearest Neighbors (KNN) Classifier\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_accuracy, knn_classification_rep = batch_train(knn_classifier, X_train_batches, y_train_batches, X_test, y_test)\n",
        "\n",
        "print(\"K-Nearest Neighbors (KNN) Classifier:\")\n",
        "print(f\"Accuracy: {knn_accuracy}\")\n",
        "print(\"Classification Report:\\n\", knn_classification_rep)\n",
        "\n",
        "# Decision Tree (DT) Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "dt_accuracy, dt_classification_rep = batch_train(dt_classifier, X_train_batches, y_train_batches, X_test, y_test)\n",
        "\n",
        "print(\"\\nDecision Tree (DT) Classifier:\")\n",
        "print(f\"Accuracy: {dt_accuracy}\")\n",
        "print(\"Classification Report:\\n\", dt_classification_rep)\n",
        "\n",
        "# Linear Support Vector Machine (SVM) Classifier\n",
        "linear_svm_classifier = SVC(kernel='linear', C=1.0)\n",
        "linear_svm_accuracy, linear_svm_classification_rep = batch_train(linear_svm_classifier, X_train_batches, y_train_batches, X_test, y_test)\n",
        "\n",
        "print(\"\\nLinear Support Vector Machine (SVM) Classifier:\")\n",
        "print(f\"Accuracy: {linear_svm_accuracy}\")\n",
        "print(\"Classification Report:\\n\", linear_svm_classification_rep)\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_accuracy, rf_classification_rep = batch_train(rf_classifier, X_train_batches, y_train_batches, X_test, y_test)\n",
        "\n",
        "print(\"\\nRandom Forest Classifier:\")\n",
        "print(f\"Accuracy: {rf_accuracy}\")\n",
        "print(\"Classification Report:\\n\", rf_classification_rep)\n",
        "\n",
        "# Gradient Boosting Classifier\n",
        "gb_classifier = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "gb_accuracy, gb_classification_rep = batch_train(gb_classifier, X_train_batches, y_train_batches, X_test, y_test)\n",
        "\n",
        "print(\"\\nGradient Boosting Classifier:\")\n",
        "print(f\"Accuracy: {gb_accuracy}\")\n",
        "print(\"Classification Report:\\n\", gb_classification_rep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFRejqAv62U6"
      },
      "outputs": [],
      "source": [
        "# # code to appy epoch\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "# from sklearn.svm import SVC\n",
        "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "# from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# def batch_train(model, X_train_batches, y_train_batches, X_test, y_test, epochs=1):\n",
        "#     for epoch in range(epochs):\n",
        "#         for X_batch, y_batch in zip(X_train_batches, y_train_batches):\n",
        "#             model.fit(X_batch, y_batch)\n",
        "\n",
        "#         y_pred = model.predict(X_test)\n",
        "#         accuracy = accuracy_score(y_test, y_pred)\n",
        "#         classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "#         print(f\"Epoch {epoch + 1} - Accuracy: {accuracy}\")\n",
        "#         print(\"Classification Report:\\n\", classification_rep)\n",
        "\n",
        "# # Assuming loaded_bert_embeddings and loaded_y_encoded are defined\n",
        "# X_train, X_test, y_train, y_test = train_test_split(loaded_bert_embeddings, loaded_y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# batch_size = 100\n",
        "# epochs = 3  # You can adjust the number of epochs\n",
        "\n",
        "# X_train_batches = [X_train[i:i+batch_size] for i in range(0, len(X_train), batch_size)]\n",
        "# y_train_batches = [y_train[i:i+batch_size] for i in range(0, len(y_train), batch_size)]\n",
        "\n",
        "# # K-Nearest Neighbors (KNN) Classifier\n",
        "# knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "# batch_train(knn_classifier, X_train_batches, y_train_batches, X_test, y_test, epochs=epochs)\n",
        "\n",
        "# # ... Repeat for other classifiers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BApQD3sTnuTc"
      },
      "outputs": [],
      "source": [
        "print(len(loaded_bert_embeddings))\n",
        "print(len(loaded_y_encoded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2LwCMAboTvL"
      },
      "outputs": [],
      "source": [
        "loaded_bert_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fqVOPTHoZuH"
      },
      "outputs": [],
      "source": [
        "loaded_y_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WZIPB1FPQq9"
      },
      "outputs": [],
      "source": [
        "# end end end  end end end  end end end  end end end  end end end  end end end  end end end  end end end\n",
        "# end end end  end end end  end end end  end end end  end end end  end end end  end end end  end end end\n",
        "# end end end  end end end  end end end  end end end  end end end  end end end  end end end  end end end\n",
        "# end end end  end end end  end end end  end end end  end end end  end end end  end end end  end end end"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
